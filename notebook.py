# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hbw7MD-QqOPC81jtsZEwQHlAas6o0rOv
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from scipy import stats
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline

# Load dataset
df = pd.read_csv('./data/regenerated_landslide_risk_dataset.csv')

# Convert target variable to numeric
df['Landslide Risk Prediction'] = df['Landslide Risk Prediction'].apply(lambda x: 0 if x == 'Low' else 1)

# Save preprocessed dataset
df.to_csv("final_dataset.csv", index=False)

# Set up plot style
plt.style.use('ggplot')
sns.set(font_scale=1.2)

# Class distribution visualization
plt.figure(figsize=(10, 6))
sns.countplot(x='Landslide Risk Prediction', data=df)
plt.title('Class Distribution (Imbalance Visualization)')
plt.savefig('./images/class_imbalance.png')
plt.close()

# Distribution of features by class
fig, axes = plt.subplots(2, 3, figsize=(20, 12))
axes = axes.flatten()

for i, feature in enumerate(df.columns[:-1]):
    if i < len(axes):  # Fix: Ensure we don't exceed available axes
        sns.boxplot(x='Landslide Risk Prediction', y=feature, data=df, ax=axes[i])
        axes[i].set_title(f'{feature} by Landslide Risk')

plt.tight_layout()
plt.savefig('./images/feature_distribution_by_class.png')
plt.close()

# Correlation heatmap
plt.figure(figsize=(12, 10))
correlation = df.corr()
mask = np.triu(correlation)
sns.heatmap(correlation, annot=True, cmap='coolwarm', mask=mask, vmin=-1, vmax=1)
plt.title('Feature Correlation Matrix')
plt.tight_layout()
plt.savefig('./images/correlation_heatmap.png')
plt.close()

# Feature distributions
fig, axes = plt.subplots(2, 3, figsize=(20, 12))
axes = axes.flatten()

for i, feature in enumerate(df.columns[:-1]):
    if i < len(axes):  # Fix: Ensure we don't exceed available axes
        sns.histplot(df[feature], kde=True, ax=axes[i])
        axes[i].set_title(f'Distribution of {feature}')

plt.tight_layout()
plt.savefig('./images/feature_distributions.png')
plt.close()

# Pairplot for feature relationships
plt.figure(figsize=(15, 15))
sns.pairplot(df, hue='Landslide Risk Prediction', corner=True)
plt.savefig('./images/pairplot.png')
plt.close()

# Feature scaling for PCA
X = df.drop('Landslide Risk Prediction', axis=1)
y = df['Landslide Risk Prediction']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA for dimensionality reduction visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

pca_df = pd.DataFrame({
    'PC1': X_pca[:, 0],
    'PC2': X_pca[:, 1],
    'Landslide Risk': y
})

plt.figure(figsize=(10, 8))
sns.scatterplot(x='PC1', y='PC2', hue='Landslide Risk',
                data=pca_df, palette=['skyblue', 'salmon'], s=100)
plt.title('PCA: First Two Principal Components')
plt.savefig('./images/pca_visualization.png')  # Fix: Add directory
plt.close()

# Statistical tests to compare features between classes
print("Feature differences between classes:")
for feature in X.columns:
    class0 = df[df['Landslide Risk Prediction'] == 0][feature]
    class1 = df[df['Landslide Risk Prediction'] == 1][feature]

    # Check if both classes have at least one value
    if len(class0) > 0 and len(class1) > 0:
        t_stat, p_val = stats.ttest_ind(class0, class1, equal_var=False)
        print(f"{feature}: p-value = {p_val:.4f} {'(significant)' if p_val < 0.05 else ''}")
    else:
        print(f"{feature}: Cannot perform t-test - insufficient data in one or both classes")

# Function to create preprocessing pipeline
def create_preprocessing_pipeline(numeric_features, categorical_features=None):
    """
    Create a scikit-learn preprocessing pipeline for numeric and categorical features
    """
    transformers = []

    # Numeric features pipeline
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    transformers.append(('num', numeric_transformer, numeric_features))

    # Categorical features pipeline (if any)
    if categorical_features and len(categorical_features) > 0:
        categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore'))
        ])
        transformers.append(('cat', categorical_transformer, categorical_features))

    # Create the preprocessor
    preprocessor = ColumnTransformer(transformers=transformers)

    return preprocessor

# Function to train and evaluate models with different resampling techniques
def train_evaluate_models(X, y, preprocessor, models_dict, cv=5):
    """
    Train and evaluate different models with different resampling strategies
    """
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # Resampling strategies
    resampling_strategies = {
        'No Resampling': None,
        'SMOTE': SMOTE(random_state=42),
        'ADASYN': ADASYN(random_state=42),
        'Undersampling': RandomUnderSampler(random_state=42)
    }

    results = {}
    models = {}

    # For visualization
    plt.figure(figsize=(20, 15))

    # Create grid based on number of strategies and models
    num_strategies = len(resampling_strategies)
    num_models = len(models_dict)
    num_rows = num_strategies
    num_cols = num_models
    
    # Create overall figure
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 15))
    if num_rows == 1 and num_cols == 1:
        axes = np.array([[axes]])  # Make it 2D
    elif num_rows == 1:
        axes = np.array([axes])  # Make it 2D
    elif num_cols == 1:
        axes = np.array([[ax] for ax in axes])  # Make it 2D

    for s_idx, (strategy_name, resampler) in enumerate(resampling_strategies.items()):
        print(f"\n{'-'*50}\nTraining models with {strategy_name}\n{'-'*50}")

        strategy_results = {}

        for m_idx, (model_name, model) in enumerate(models_dict.items()):
            print(f"\nTraining {model_name}...")

            # Create pipeline with or without resampling
            if resampler:
                pipeline = ImbPipeline([
                    ('preprocessor', preprocessor),
                    ('resampler', resampler),
                    ('classifier', model)
                ])
            else:
                pipeline = Pipeline([
                    ('preprocessor', preprocessor),
                    ('classifier', model)
                ])

            # Train and evaluate the model
            pipeline.fit(X_train, y_train)
            y_pred = pipeline.predict(X_test)
            
            # Fix: Check if the model has predict_proba method
            if hasattr(pipeline, 'predict_proba'):
                y_pred_proba = pipeline.predict_proba(X_test)[:, 1]
            else:
                # For models that don't have predict_proba, use decision_function if available
                if hasattr(pipeline, 'decision_function'):
                    y_pred_proba = pipeline.decision_function(X_test)
                    # Normalize to [0,1] range
                    y_pred_proba = (y_pred_proba - y_pred_proba.min()) / (y_pred_proba.max() - y_pred_proba.min())
                else:
                    # If neither is available, use the predictions as is
                    y_pred_proba = y_pred

            # Calculate metrics
            f1 = f1_score(y_test, y_pred)

            # Calculate ROC curve and AUC
            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
            roc_auc = auc(fpr, tpr)

            # Calculate Precision-Recall curve and AUC
            precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
            pr_auc = average_precision_score(y_test, y_pred_proba)

            # Store results
            strategy_results[model_name] = {
                'pipeline': pipeline,
                'f1_score': f1,
                'roc_auc': roc_auc,
                'pr_auc': pr_auc,
                'fpr': fpr,
                'tpr': tpr,
                'precision': precision,
                'recall': recall,
                'y_pred': y_pred,
                'y_pred_proba': y_pred_proba
            }

            print(f"F1 Score: {f1:.4f}")
            print(f"ROC AUC: {roc_auc:.4f}")
            print(f"PR AUC: {pr_auc:.4f}")
            print("\nClassification Report:")
            print(classification_report(y_test, y_pred))

            # Plot confusion matrix
            ax = axes[s_idx, m_idx]
            conf_matrix = confusion_matrix(y_test, y_pred)
            sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)
            ax.set_title(f"{strategy_name} - {model_name}\nConfusion Matrix")
            ax.set_ylabel('True Label')
            ax.set_xlabel('Predicted Label')

            # Save the model
            models[f"{strategy_name}_{model_name}"] = pipeline

        results[strategy_name] = strategy_results

    plt.tight_layout()
    plt.savefig('./images/confusion_matrices.png')  # Fix: Add directory
    plt.close()

    # Plot ROC curves
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()

    for i, (strategy_name, strategy_results) in enumerate(results.items()):
        if i < len(axes):  # Fix: Check if we have enough axes
            ax = axes[i]
            for model_name, metrics in strategy_results.items():
                ax.plot(metrics['fpr'], metrics['tpr'], label=f"{model_name} (AUC = {metrics['roc_auc']:.2f})")

            ax.plot([0, 1], [0, 1], 'k--')
            ax.set_xlabel('False Positive Rate')
            ax.set_ylabel('True Positive Rate')
            ax.set_title(f'ROC Curve - {strategy_name}')
            ax.legend(loc='lower right')

    plt.tight_layout()
    plt.savefig('./images/roc_curves.png')  # Fix: Add directory
    plt.close()

    # Plot Precision-Recall curves
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()

    for i, (strategy_name, strategy_results) in enumerate(results.items()):
        if i < len(axes):  # Fix: Check if we have enough axes
            ax = axes[i]
            for model_name, metrics in strategy_results.items():
                ax.plot(metrics['recall'], metrics['precision'], label=f"{model_name} (AUC = {metrics['pr_auc']:.2f})")

            ax.set_xlabel('Recall')
            ax.set_ylabel('Precision')
            ax.set_title(f'Precision-Recall Curve - {strategy_name}')
            ax.legend(loc='lower left')

    plt.tight_layout()
    plt.savefig('./images/pr_curves.png')  # Fix: Add directory
    plt.close()

    # Plot feature importance for tree-based models
    for strategy_name, strategy_results in results.items():
        for model_name, metrics in strategy_results.items():
            if model_name in ['Random Forest', 'Gradient Boosting', 'XGBoost']:
                pipeline = metrics['pipeline']
                
                # Fix: Check if classifier has feature_importances_
                if not hasattr(pipeline['classifier'], 'feature_importances_'):
                    print(f"Model {model_name} does not have feature_importances_ attribute. Skipping.")
                    continue

                # Get feature names after preprocessing
                try:
                    feature_names = pipeline['preprocessor'].get_feature_names_out()
                except (AttributeError, ValueError):
                    # Fallback for older sklearn versions or when get_feature_names_out fails
                    feature_names = [f'feature_{i}' for i in range(pipeline['classifier'].feature_importances_.shape[0])]

                # Get feature importances
                importances = pipeline['classifier'].feature_importances_

                # Plot
                plt.figure(figsize=(10, 6))
                indices = np.argsort(importances)[::-1]
                plt.title(f'Feature Importances - {strategy_name} - {model_name}')
                num_features = min(10, len(indices))
                plt.barh(range(num_features), importances[indices][:num_features], align='center')
                plt.yticks(range(num_features), [feature_names[i] for i in indices][:num_features])
                plt.xlabel('Relative Importance')
                plt.tight_layout()
                plt.savefig(f'./images/feature_importance_{strategy_name}_{model_name}.png')  # Fix: Add directory
                plt.close()

    return results, models, X_test, y_test

# Main execution
if __name__ == "__main__":
    # Check and create directories for saved images
    import os
    os.makedirs('./images', exist_ok=True)

    # Define features and target
    X = df.drop('Landslide Risk Prediction', axis=1)
    y = df['Landslide Risk Prediction']

    # Check class imbalance
    print("\nClass distribution:")
    print(y.value_counts())
    class_counts = y.value_counts()
    
    # Fix: Handle potential division by zero or wrong order
    if len(class_counts) >= 2:
        minority_class = min(class_counts.index, key=lambda x: class_counts[x])
        majority_class = max(class_counts.index, key=lambda x: class_counts[x])
        if class_counts[minority_class] > 0:
            imbalance_ratio = class_counts[majority_class] / class_counts[minority_class]
            print(f"Class imbalance ratio: 1:{imbalance_ratio:.2f}")
        else:
            print("Warning: Minority class has zero samples")
    else:
        print("Warning: Only one class present in the dataset")

    # Define feature types
    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()

    # Create preprocessing pipeline
    preprocessor = create_preprocessing_pipeline(numeric_features, categorical_features)

    # Define models to try - Fix: Remove use_label_encoder parameter which is deprecated
    models_dict = {
        'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
        'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
        'SVM': SVC(probability=True, class_weight='balanced', random_state=42),
        'XGBoost': XGBClassifier(eval_metric='logloss', random_state=42)  # Removed use_label_encoder
    }

    # Train and evaluate models
    results, models, X_test, y_test = train_evaluate_models(X, y, preprocessor, models_dict)

    # Find best model based on F1 score
    best_f1 = 0
    best_model_name = ""
    best_strategy = ""

    for strategy_name, strategy_results in results.items():
        for model_name, metrics in strategy_results.items():
            if metrics['f1_score'] > best_f1:
                best_f1 = metrics['f1_score']
                best_model_name = model_name
                best_strategy = strategy_name

    print(f"\nBest model: {best_model_name} with {best_strategy}, F1 Score: {best_f1:.4f}")
    
    # Save the best model
    from skl2onnx import convert_sklearn
    from skl2onnx.common.data_types import FloatTensorType  
    # Assuming you have trained the best model using your pipeline or strategy
    best_model = models[f"{best_strategy}_{best_model_name}"]   
    # Define the feature names and expected input type
    # You should specify your feature columns here to match what the model expects
    print("Best model features:", best_model)
    initial_types = [('Temperature (Â°C)', FloatTensorType([None, 1])),
                      ('Humidity (%)', FloatTensorType([None, 1])),
                      ( 'Precipitation (mm)', FloatTensorType([None, 1])),
                        ( 'Soil Moisture (%)', FloatTensorType([None, 1])),
                        ('Elevation (m)', FloatTensorType([None, 1])),
                    ('input', FloatTensorType([None, best_model.n_features_in_]))]  
    # Convert the model to ONNX format
    onnx_model = convert_sklearn(best_model, 
                                initial_types=initial_types)    
    # Save the ONNX model to a file
    with open('best_landslide_model.onnx', 'wb') as f:
        f.write(onnx_model.SerializeToString()) 
    print("Best model saved as 'best_landslide_model.onnx'")    